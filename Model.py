import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq
from transformers import AutoProcessor, AutoModelForImageTextToText,AutoModelForZeroShotObjectDetection
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from Tips import *
import xml.etree.ElementTree as ET
import json
from dashscope import MultiModalConversation
import requests
import numpy as np
from io import BytesIO
from transformers import pipeline
from openai import OpenAI
import base64
from transformers import CLIPModel, CLIPProcessor
from scipy.spatial.distance import cosine
from volcenginesdkarkruntime import Ark
import random
######################################æŒ‡å®šè®¾å¤‡
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
#######################################
class Expert():
    def __init__(self,processor=None,model=None,generator=None):
        self.processor=processor
        self.model=model
        self.generator=generator
    def ToDevice(self,device):
        to_list=[self.processor,self.model,self.generator]
        for x in to_list:
            if x:
                x.to(device)
        if self.model:
            self.model.eval()
##############å®šä¹‰å„ç§æ¨¡å‹
LLM=None    #ä½¿ç”¨deepseek-r1ä½œä¸ºLLM
VLM=None    #ä½¿ç”¨GLM4.1ä½œä¸ºVLM
Editor=None #å›¾åƒç¼–è¾‘æ¨¡å‹æš‚æ—¶æ²¡æœ‰
GroundingDINO=None
SAM=None
CLIP=None
######################################
######################################å¼‚æ­¥åŠ è½½æ‰€æœ‰æ¨¡å‹
def LoadAllModel():
    def LoadLLM():
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,  # å¯ç”¨ 4bit é‡åŒ–
            bnb_4bit_use_double_quant=True,  # åŒé‡åŒ–ï¼Œè¿›ä¸€æ­¥å‡å°‘æ˜¾å­˜å ç”¨
            bnb_4bit_quant_type="nf4",  # æ¨èçš„é‡åŒ–ç±»å‹ï¼ˆæ¯” fp4 æ›´é€‚åˆè‡ªç„¶è¯­è¨€ï¼‰
            bnb_4bit_compute_dtype=torch.bfloat16  # è®¡ç®—æ—¶ç”¨ bfloat16ï¼Œä¿ç²¾åº¦
        )
        processor = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-R1-0528-Qwen3-8B")
        model = AutoModelForCausalLM.from_pretrained(
            "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
            quantization_config=bnb_config, 
            device_map="auto", 
            trust_remote_code=True
        ).to(DEVICE).eval()
        global LLM
        LLM=Expert(processor,model)
    def LoadVLM():
        QUANT_CONFIG = {
            "load_in_4bit": False,
            "load_in_8bit": False,
            "low_cpu_mem_usage": False,
        }
        processor = AutoProcessor.from_pretrained("zai-org/GLM-4.1V-9B-Thinking")
        model = AutoModelForImageTextToText.from_pretrained(
            "zai-org/GLM-4.1V-9B-Thinking",
            trust_remote_code=True,  # å¿…é¡»å¼€å¯ï¼ˆå¤šæ¨¡æ€æ¨¡å‹ç»“æ„éœ€è¿œç¨‹ä»£ç ï¼‰
            device_map="auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡ï¼ˆä¼˜å…ˆç”¨GPUï¼Œå‰©ä½™æ”¾CPUï¼‰
            **QUANT_CONFIG  # å¯ç”¨æ˜¾å­˜ä¼˜åŒ–ï¼ˆè‹¥CPUè¿è¡Œï¼Œåˆ é™¤è¿™ä¸€è¡Œï¼‰
            ).to(DEVICE).eval()
        global VLM
        VLM=Expert(processor,model)
    def LoadGroundingDINO():
        processor = AutoProcessor.from_pretrained("IDEA-Research/grounding-dino-base")
        model = AutoModelForZeroShotObjectDetection.from_pretrained("IDEA-Research/grounding-dino-base").to(DEVICE)
        global GroundingDINO
        GroundingDINO=Expert(processor,model)  
    def LoadSAM():
        generator = pipeline("mask-generation", model="facebook/sam2.1-hiera-large", device=0)
        global SAM
        SAM=Expert(generator=generator)
    def LoadClip():
        model_name = "openai/clip-vit-large-patch14"
        processor = CLIPProcessor.from_pretrained(model_name)
        model = CLIPModel.from_pretrained(model_name).to(DEVICE).eval()
        global CLIP
        CLIP=Expert(processor=processor,model=model)
    #å¤šçº¿ç¨‹åŠ è½½
    tasks=[
        #    LoadVLM,
        #    LoadLLM,
            LoadGroundingDINO,
            LoadSAM,
            LoadClip,
        ]
    for task in tasks:
        task()
    Debug("æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæ¯•")
######################################
def ExtractAnswer(data:str):
        think=""
        answer=""
        beg0=data.find("<answer>")
        end0=data.find("</answer>") 
        if beg0!=-1 and end0!=-1:
            answer=data[beg0+len("<answer>"):end0]
        beg1=data.find("<think>")
        end1=data.find("</think>")
        if beg1!=-1 and end1!=-1:
            think=data[beg1+len("<think>"):end1]
        return think,answer
#######################################ç¼–ç å›¾ç‰‡
def encode_image(pil_image):
    buffer = BytesIO()
    pil_image.save(buffer, format="JPEG")
    encoded_string = base64.b64encode(buffer.getvalue()).decode('utf-8')
    return f"data:{'image/jpeg'};base64,{encoded_string}"
#######################################
client = Ark(
    # æ­¤ä¸ºé»˜è®¤è·¯å¾„ï¼Œæ‚¨å¯æ ¹æ®ä¸šåŠ¡æ‰€åœ¨åœ°åŸŸè¿›è¡Œé…ç½®
    base_url="https://ark.cn-beijing.volces.com/api/v3",
    # ä»ç¯å¢ƒå˜é‡ä¸­è·å–æ‚¨çš„ API Keyã€‚æ­¤ä¸ºé»˜è®¤æ–¹å¼ï¼Œæ‚¨å¯æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹
    api_key="723cff33-3b13-420d-ab6d-267800a27475",
    timeout=1800,
    # è®¾ç½®é‡è¯•æ¬¡æ•°
    max_retries=2,
)
def AnswerText(question:str):
    ##########################è°ƒç”¨è±†åŒ…å¤§æ¨¡å‹
    try:
        # Non-streaming:
        completion = client.chat.completions.create(
            # æŒ‡å®šæ‚¨åˆ›å»ºçš„æ–¹èˆŸæ¨ç†æ¥å…¥ç‚¹ IDï¼Œæ­¤å¤„å·²å¸®æ‚¨ä¿®æ”¹ä¸ºæ‚¨çš„æ¨ç†æ¥å…¥ç‚¹ ID
            model="doubao-1-5-pro-256k-250115",
            messages=[
                {"role": "user", "content": f"{question}"},
            ],
        )
        return completion.choices[0].message.content
    except Exception as e:
        Debug(e)
        return AnswerText(question)
    ############################
        processor=LLM.processor
        model=LLM.model
        # 4. å®šä¹‰å¯¹è¯å†…å®¹
        messages = [
            {"role": "user", "content":"{}".format(question)},
        ]

        # 5. å¤„ç†è¾“å…¥ï¼ˆè½¬ä¸ºæ¨¡å‹å¯è¯†åˆ«çš„æ ¼å¼ï¼Œå¹¶ç§»åˆ°æ¨¡å‹æ‰€åœ¨è®¾å¤‡ï¼‰
        inputs = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            enable_thinking=False,
            return_tensors="pt",
        ).to(DEVICE)  # ç¡®ä¿è¾“å…¥ä¸æ¨¡å‹åœ¨åŒä¸€è®¾å¤‡ï¼ˆGPUï¼‰

        # 6. ç”Ÿæˆå›å¤ï¼ˆæ·»åŠ æ¨ç†å‚æ•°ä¼˜åŒ–æ•ˆæœï¼‰
        outputs = model.generate(
            **inputs,
            max_new_tokens=65535,  # æœ€å¤§ç”Ÿæˆ token æ•°
            temperature=0.7,  # éšæœºæ€§ï¼ˆ0-1ï¼Œè¶Šä½è¶Šç¡®å®šï¼‰
            do_sample=True,  # å¯ç”¨é‡‡æ ·ç”Ÿæˆï¼ˆæ›´è‡ªç„¶ï¼‰
            pad_token_id=processor.eos_token_id  # å¡«å…… token æŒ‡å®š
        )

        # 7. è§£ç å¹¶æ‰“å°å›å¤ï¼ˆåªå–æ–°å¢ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        response = processor.decode(
            outputs[0][inputs["input_ids"].shape[-1]:],  # è·³è¿‡è¾“å…¥éƒ¨åˆ†
            skip_special_tokens=True  # å¿½ç•¥ç‰¹æ®Š tokenï¼ˆå¦‚ <bos> <eos>ï¼‰
        )
        return response
##########################

def AnswerImage(images:list,text:str):
    #####################
    try:
        response = client.chat.completions.create(
        # æŒ‡å®šæ‚¨åˆ›å»ºçš„æ–¹èˆŸæ¨ç†æ¥å…¥ç‚¹ IDï¼Œæ­¤å¤„å·²å¸®æ‚¨ä¿®æ”¹ä¸ºæ‚¨çš„æ¨ç†æ¥å…¥ç‚¹ ID
        model="doubao-seed-1-6-vision-250815",
        messages=[
            {
                "role": "user",
                "content": [
                        {"type": "text", "text": f"{text}"},
                ]+
                [ {"type": "image_url", "image_url": {"url": encode_image(image)}} for image in images]
                ,
            }
        ],
        )
        return (response.choices[0].message.content)
    except Exception as e:
        Debug(e)
        return AnswerImage(images,text)
    #####################
    processor=VLM.processor
    model=VLM.model
        #
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image"} for _ in range(len(images))
                ]+[{"type": "text", "text": text}]
        },
    ]
    # Prepare inputs
    prompt = processor.apply_chat_template(messages, add_generation_prompt=True)
    inputs = processor(text=prompt, images=images, return_tensors="pt")
    inputs = inputs.to(DEVICE)
    # Generate outputs
    generated_ids = model.generate(**inputs, max_new_tokens=65535)
    generated_texts = processor.batch_decode(
        generated_ids,
        skip_special_tokens=True,
    )
    #å¯¹å›å¤è¿›è¡Œå¤„ç†
    res=generated_texts[0]
    #   
    _,res=ExtractAnswer(res)
    return res
###############################ç»™å®šæŒ‡ä»¤è¿›è¡Œç¼–è¾‘
def EditImage(image,description:str):
    imagesResponse = client.images.generate(
        model="doubao-seededit-3-0-i2i-250628",
        prompt=description,
        image=encode_image(image),
        seed=random.randint(119,65536),
        guidance_scale=8.0,
        size="adaptive",
        watermark=True
    )
    #ä¸‹è½½å›¾ç‰‡
    image_url=imagesResponse.data[0].url
    headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
    response = requests.get(image_url, headers=headers, timeout=30)
    response.raise_for_status() #æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
    #å°†äºŒè¿›åˆ¶æ•°æ®è½¬æ¢ä¸ºPIL Imageå¯¹è±¡
    image = Image.open(BytesIO(response.content))
    return image.convert("RGB")
###############################åˆ‡å‰²åŒºåŸŸ
def ClipScore(image, target):
    processor=CLIP.processor
    model=CLIP.model
    #
    with torch.no_grad():
        inputs = processor(images=image, return_tensors="pt").to(DEVICE)
        embedding_0 = model.get_image_features(** inputs)
        inputs =processor(text=target,return_tensors="pt").to(DEVICE)
        embedding_1=model.get_text_features(**inputs)
    imageD=embedding_0.cpu().numpy().flatten()
    textD=embedding_1.cpu().numpy().flatten()
    imageD = imageD / np.linalg.norm(imageD)
    textD = textD / np.linalg.norm(textD)
    return 1.0-cosine(imageD, textD)
#ç²—åˆ†åŒºåŸŸ
def GroundingDINOForImage(image,target_obj:str):
    #
    processor=GroundingDINO.processor
    model=GroundingDINO.model 
    #
    inputs = processor(images=image, text=target_obj, return_tensors="pt").to(DEVICE)
    with torch.no_grad():
        outputs = model(**inputs)
    #å®šä¹‰é˜ˆå€¼è¡°å‡ç³»æ•°
    text_threshold_minus=0.05
    box_threshold_minus=0.05
    #æ‰¾åˆ°ç»“æœ
    def GetTarget(text_threshold,box_threshold):
        #å¦‚æœä»»æ„ä¸€ä¸ªé˜ˆå€¼å°äº0.0ç›´æ¥è¿”å›None
        if text_threshold<0.0 or box_threshold<0.0:
            return None
        #
        results = processor.post_process_grounded_object_detection(
            outputs,
            inputs.input_ids,
            threshold=box_threshold,
            text_threshold=text_threshold,
            target_sizes=[image.size[::-1]]
        )
        #è£å‰ªå‡ºæŒ‡å®šåŒºåŸŸ
        try:
            maxscore=0.0
            target=None
            targetbox=None
            boxes= results[0]['boxes'].cpu().tolist()
            for box in boxes:
                x1, y1, x2, y2 = map(int, box)  # è½¬ä¸ºæ•´æ•°
                cropped_image = image.crop((x1, y1, x2, y2))
                score=ClipScore(cropped_image,target_obj)
                if score>maxscore:
                    maxscore=score
                    if maxscore>=GroundingDINOClipScoreThreold:
                        target=cropped_image
                        targetbox=[x1,y1,x2,y2]
            #å¦‚æœæ²¡æœ‰ç»“æœ,é‚£ä¹ˆç›´æ¥æŠ¥é”™
            if target is None:
                raise Exception("æœªæ¡†å‡ºä»»ä½•ç‰©ä½“!")
            #
            Debug(f"GroundingDINOæœ€å¤§è¯„åˆ†ä¸º:{maxscore}")
            return target,targetbox
        except Exception as e:
            return GetTarget(text_threshold-text_threshold_minus,box_threshold-box_threshold_minus)
    #
    return GetTarget(0.8,0.8)
#ç»†åˆ†åŒºåŸŸ
def SAMForImage(image):
    generator=SAM.generator
    #
    image_array = np.array(image)
    H, W = image_array.shape[:2]
    input_boxes = [[[W//2, H//2, W//2+10, H//2+10]]]  
    outputs = generator(image, input_boxes=input_boxes)
    masks = outputs["masks"]  

    for i, mask in enumerate(masks):
        # ğŸ” å…³é”®ä¿®å¤ï¼šTensor â†’ NumPy
        mask_np = mask.cpu().numpy() if isinstance(mask, torch.Tensor) else mask
        mask_bool = mask_np>0.0  # è½¬ä¸ºå¸ƒå°”å‹

        # åˆ›å»º RGBA å›¾åƒ
        h, w = mask_bool.shape
        extracted = np.zeros((h, w, 4), dtype=np.uint8)
        
        # å‰æ™¯ï¼šåŸå›¾ * mask
        extracted[:, :, :3] = image_array * mask_bool[:, :, np.newaxis]  # âœ… ç°åœ¨éƒ½æ˜¯ numpy
        # Alpha é€šé“
        extracted[:, :, 3] = mask_bool * 255

        # è½¬ä¸º PIL å›¾åƒ
        cropped_img = Image.fromarray(extracted, mode="RGBA")

        # ã€å¯é€‰ã€‘è£å‰ªåˆ°ç‰©ä½“è¾¹ç•Œ
        coords = np.where(mask_bool)
        if len(coords[0]) == 0:
            continue
        top, left = coords[0].min(), coords[1].min()
        bottom, right = coords[0].max(), coords[1].max()

        # åŠ ç‚¹ margin
        margin = 10
        top = max(0, top - margin)
        left = max(0, left - margin)
        bottom = min(h, bottom + margin)
        right = min(w, right + margin)
        
        cropped_img = cropped_img.crop((left, top, right, bottom))
        cropped_img.save(f"tmp/object_{i}.png")
    target=int(input("è¯·é€‰æ‹©ç¬¦åˆçš„:"))
    target_mask = masks[target]
    mask_np = target_mask.cpu().numpy() if isinstance(target_mask, torch.Tensor) else target_mask
    return mask_np>0.0
#æå–åŒºåŸŸ
def ExtractByMask(image,mask):
    # è·å–å›¾åƒå®½é«˜
    width, height = image.size
    # åˆ›å»ºä¸€ä¸ªç›¸åŒå¤§å°çš„é€æ˜å›¾åƒï¼ˆRGBAæ¨¡å¼ï¼‰
    result = Image.new('RGBA', (width, height), (0, 0, 0, 0))
    # è·å–åƒç´ 
    image_pixels = image.load()
    result_pixels = result.load()
    # éå†æ¯ä¸ªåƒç´ 
    for w in range(width):
        for h in range(height):
            # å¦‚æœé®ç½©ä¸ºTrueï¼Œåˆ™ä¿ç•™åŸåƒç´ 
            if mask[h, w]:  
                #ä¿è¯å›¾ç‰‡è‚¯å®šæ˜¯rgbæ¨¡å¼
                rgb = image_pixels[w, h]
                result_pixels[w, h] = (*rgb, 255)
    return result.convert("RGB")
            
#è·å–ç¼–è¾‘åçš„æ‰“åˆ†
def GetImageScore(source,target,description:str):
    res=AnswerImage([source,target],description)
    score=-1
    prompt=""
    try:
        data = json.loads(res)
        if "score" in data:
            score=int(data["score"])
        if "prompt" in data:
            prompt=data["prompt"]
    except Exception as e:
        Debug(e)
        pass
    return score,prompt
#è·å–ç¼–è¾‘åçš„å±€éƒ¨æ‰“åˆ†
def GetImageLocalScore(source,target,description:str):
    res=GetImageScore(source,target,LoalScore_Prompt.format(description))
    #å¦‚æœæ˜¯å› ä¸ºæ¨¡å‹æ¡†ä½çš„åŒºåŸŸä¸åˆé€‚ï¼Œé‚£ä¹ˆç›´æ¥ç»™æ»¡åˆ†å³å¯
    if res==-1:
        Debug("æ‰€é€‰åŒºåŸŸæœ‰é—®é¢˜,ç›´æ¥ç»™å®šæ»¡åˆ†")
        return 10
#è·å–ç¼–è¾‘åçš„å…¨å±€æ‰“åˆ†
def GetImageGlobalScore(source,target,description:str):
    return GetImageScore(source,target,GlobalScore_Prompt.format(description))
#æŒ‡ä»¤ä¼˜åŒ–
def OptmEditInstruction(negPrompt:str,instruction:str):
    res=AnswerText(InsOptim_Prompt.format(negPrompt,instruction))
    try:
        data=json.loads(res)
        return data["new_instruction"]
    except Exception as e:
        return ""
#è‰ºæœ¯å®¶æ‰“åˆ†
def GetCriticScore(source,target,instructions:list):
    instruction=""
    for idx in range(len(instructions)):
        ins=instructions[idx]
        instruction+="({})".format(idx+1)+ins+"\n"
    return GetImageScore(source,target,Critic_Prompt.format(instruction))
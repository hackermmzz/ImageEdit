import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq
from transformers import AutoProcessor, AutoModelForImageTextToText,AutoModelForZeroShotObjectDetection
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from Tips import *  # ä¿ç•™ä½ çš„è‡ªå®šä¹‰å·¥å…·ç±»
import xml.etree.ElementTree as ET
import json
from dashscope import MultiModalConversation
import requests
import numpy as np
from io import BytesIO
from transformers import pipeline
from transformers import AutoProcessor, AutoModelForImageTextToText
from openai import OpenAI
import cv2
from transformers import CLIPModel, CLIPProcessor
from scipy.spatial.distance import cosine
# Load model directly
DEVICE="cuda"
######################################åŠ è½½SAM
LLM=None
class Expert():
    def __init__(self,processor=None,model=None,generator=None):
        self.processor=processor
        self.model=model
        self.generator=generator
    def ToDevice(self,device):
        to_list=[self.processor,self.model,self.generator]
        for x in to_list:
            if x and hasattr(x, 'to'):  # ä¿®å¤ï¼šä»…å¯¹æœ‰to()æ–¹æ³•çš„å¯¹è±¡ç§»è®¾å¤‡ï¼ˆå¦‚modelï¼‰
                x.to(device)
        if self.model:
            self.model.eval()

def AnswerText(question:str):
        print(question)
        processor=LLM.processor
        model=LLM.model
        # 4. å®šä¹‰å¯¹è¯å†…å®¹
        messages = [
            {"role": "user", "content":"{}".format(question)},
        ]

        # 5. å¤„ç†è¾“å…¥ï¼ˆè½¬ä¸ºæ¨¡å‹å¯è¯†åˆ«çš„æ ¼å¼ï¼Œå¹¶ç§»åˆ°æ¨¡å‹æ‰€åœ¨è®¾å¤‡ï¼‰
        inputs = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            enable_thinking=False,
            return_tensors="pt",
        ).to(DEVICE)

        # 6. ç”Ÿæˆå›å¤ï¼ˆæ·»åŠ æ¨ç†å‚æ•°ä¼˜åŒ–æ•ˆæœï¼‰
        with torch.no_grad():  # æ–°å¢ï¼šå‡å°‘æ˜¾å­˜å ç”¨ï¼Œé¿å…æ¢¯åº¦è®¡ç®—
            outputs = model.generate(
                **inputs,
                max_new_tokens=65535,
                temperature=0.7,
                do_sample=True,
                pad_token_id=processor.eos_token_id
            )

        # 7. è§£ç å¹¶æ‰“å°å›å¤
        response = processor.decode(
            outputs[0][inputs["input_ids"].shape[-1]:],
            skip_special_tokens=True
        )
        return response

def LoadLLM():
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,  # è¡¥å……4bité‡åŒ–é…ç½®ï¼ˆåŸä»£ç ç¼ºå¤±ï¼Œé¿å…æ˜¾å­˜æº¢å‡ºï¼‰
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )
        processor = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-R1-0528-Qwen3-8B")
        model = AutoModelForCausalLM.from_pretrained(
            "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B",
            quantization_config=bnb_config, 
            device_map="auto", 
            trust_remote_code=True
        ).eval()  # å·²åŒ…å«to(device)ï¼Œæ— éœ€é‡å¤è°ƒç”¨
        global LLM
        LLM=Expert(processor,model)

def GetChange(scene:str,tasks:list):
    def AnswerText(text):
        client = OpenAI(
            base_url="https://ark.cn-beijing.volces.com/api/v3",
            api_key="723cff33-3b13-420d-ab6d-267800a27475",
        )
        completion = client.chat.completions.create(
            model="doubao-1-5-pro-256k-250115",
            messages=[{"role": "user", "content": f"{text}"}],
        )
        return completion.choices[0].message.content
        
    text="".join([f"({i+1}){task}\n" for i,task in enumerate(tasks)])
    answer=AnswerText(Expert4_Prompt.format(scene,text))
    try:
        ret=[change.split(":") for change in json.loads(answer)]
        return [[s[0].lower(),s[1].lower()] for s in ret]
    except Exception as e:
        return GetChange(scene,tasks)

def RefineTasks(scene :str,tasks:list):
        text="".join([f"({i+1}){task}\n" for i,task in enumerate(tasks)])
        answer=AnswerText(Expert3_Prompt.format(scene,text))
        try:
            return json.loads(answer)
        except Exception as e:
            return RefineTasks(scene,tasks)

scene_json='''
    {
    "global": {
        "scene_type": "natural, wooded environment",
        "background": "blurred green foliage indicating a lush forest or jungle setting with diffused natural light",
        "main_elements": "tree branches and a small bird as central subjects",
        "lighting": "natural daylight with soft, diffused illumination"
    },
    "local": {
        "objects": [
            {
                "type": "bird",
                "details": "small bird with green body plumage, red patch on head, black beak, perched on a tree branch or wooden structure"
            },
            {
                "type": "tree branches",
                "details": "thick, textured branches; one large branch in foreground with blurred (bokeh) effect, another branch the bird is on with visible wood texture"
            }
        ],
        "colors": {
            "bird": "green body, red head patch, black beak",
            "tree": "brown bark with textured surface",
            "background": "various shades of green from foliage"
        },
        "other_details": "no human presence; focus on wildlife in natural habitat with shallow depth of field emphasizing the bird"
        }
    }
'''

GroundingDINO=None
def LoadGroundingDINO():
        processor = AutoProcessor.from_pretrained("IDEA-Research/grounding-dino-base")
        model = AutoModelForZeroShotObjectDetection.from_pretrained("IDEA-Research/grounding-dino-base").to(DEVICE).eval()
        global GroundingDINO
        GroundingDINO=Expert(processor,model)  
#----------------------------------------------------------------------------------------#
model_name = "openai/clip-vit-large-patch14"
processor = CLIPProcessor.from_pretrained(model_name)
model = CLIPModel.from_pretrained(model_name).to(DEVICE).eval()
# -------------------------- æ”¹åŠ¨2ï¼šä¿®å¤CLIPæ‰“åˆ†å‡½æ•°ï¼ˆè‡ªåŠ¨åŠ å¯¹æ¯”æ ‡ç­¾ï¼‰ --------------------------
def get_target_score(image, target):
    with torch.no_grad():
        inputs = processor(images=image, return_tensors="pt").to(DEVICE)
        embedding_0 = model.get_image_features(** inputs)
        inputs =processor(text=target,return_tensors="pt").to(DEVICE)
        embedding_1=model.get_text_features(**inputs)
    imageD=embedding_0.cpu().numpy().flatten()
    textD=embedding_1.cpu().numpy().flatten()
    imageD = imageD / np.linalg.norm(imageD)
    textD = textD / np.linalg.norm(textD)
    return 1.0-cosine(imageD, textD)
# -------------------------- æ”¹åŠ¨1ï¼šä¿®å¤GroundingDINOï¼Œæ–°å¢CLIPç­›é€‰é€»è¾‘ --------------------------
def GroundingDINOWithCLIP(image, target:str):
     #
    processor=GroundingDINO.processor
    model=GroundingDINO.model 
    #
    #å®šä¹‰é˜ˆå€¼è¡°å‡ç³»æ•°
    text_threshold_minus=0.05
    box_threshold_minus=0.05
    inputs = processor(images=image, text=target, return_tensors="pt").to(DEVICE)
    #æ‰¾åˆ°ç»“æœ
    def GetTarget(text_threshold,box_threshold):
        #å¦‚æœä»»æ„ä¸€ä¸ªé˜ˆå€¼å°äº0.0ç›´æ¥è¿”å›None
        if text_threshold<0.0 or box_threshold<0.0:
            return None
        #
        with torch.no_grad():
            outputs = model(**inputs)
        results = processor.post_process_grounded_object_detection(
            outputs,
            inputs.input_ids,
            threshold=box_threshold,
            text_threshold=text_threshold,
            target_sizes=[image.size[::-1]]
        )
        #è£å‰ªå‡ºæŒ‡å®šåŒºåŸŸ
        try:
            maxscore=0.0
            ret=None
            boxes= results[0]['boxes'].cpu().tolist()
            for box in boxes:
                x1, y1, x2, y2 = map(int, box)  # è½¬ä¸ºæ•´æ•°
                try:
                    cropped_image = image.crop((x1, y1, x2, y2))
                    score=get_target_score(cropped_image,target)
                    cropped_image.save(f"debug/{target}_{score}.png")
                    if score>maxscore:
                        maxscore=score
                        if maxscore>GroundingDINOClipScoreThreold:
                            ret=cropped_image
                except Exception as e:
                    continue
            #å¦‚æœæ²¡æœ‰ç»“æœ,é‚£ä¹ˆç›´æ¥æŠ¥é”™
            print(maxscore)
            if ret is None:
                raise Exception("æœªæ¡†å‡ºä»»ä½•ç‰©ä½“!")
            #
            return ret
        except Exception as e:
            return GetTarget(text_threshold-text_threshold_minus,box_threshold-box_threshold_minus)
    #
    return GetTarget(0.8,0.8)

# -------------------------- æ”¹åŠ¨3ï¼šä¼˜åŒ–SAMï¼Œç”¨CLIPç­›é€‰åçš„æ¡†ä½œä¸ºæç¤ºï¼ˆæ— éœ€æ‰‹åŠ¨é€‰æ©ç ï¼‰ --------------------------
def SAMForImage(image, target:str, confidence_threshold=0.5):
    """
    ä¼˜åŒ–ï¼šç”¨CLIP+GroundingDINOçš„ç»“æœä½œä¸ºSAMæç¤ºï¼Œè‡ªåŠ¨ç”Ÿæˆç²¾å‡†åˆ†å‰²æ©ç 
    """
    # 1. å…ˆé€šè¿‡CLIP+GroundingDINOè·å–ç²¾å‡†ç›®æ ‡æ¡†
    image = GroundingDINOWithCLIP(image, target)
    image.save("debug/ground-dino.png")
    # 2. åŠ è½½SAMæ¨¡å‹ï¼ˆä¿æŒä½ çš„åŸæœ‰é…ç½®ï¼Œæ–°å¢ç¼“å­˜è·¯å¾„é¿å…é‡å¤ä¸‹è½½ï¼‰
    generator = pipeline(
        "mask-generation", 
        model="facebook/sam2.1-hiera-large", 
        device=DEVICE,
        points_per_batch=64,
        mask_threshold=confidence_threshold,
    )

   #
    image_array = np.array(image)
    H, W = image_array.shape[:2]
   # input_boxes = [[best_box]]  
    outputs = generator(image)
    masks = outputs["masks"]  
    for i, mask in enumerate(masks):
        # ğŸ” å…³é”®ä¿®å¤ï¼šTensor â†’ NumPy
        mask_np = mask.cpu().numpy() if isinstance(mask, torch.Tensor) else mask
        mask_bool = mask_np>0.0  # è½¬ä¸ºå¸ƒå°”å‹

        # åˆ›å»º RGBA å›¾åƒ
        h, w = mask_bool.shape
        extracted = np.zeros((h, w, 4), dtype=np.uint8)
        
        # å‰æ™¯ï¼šåŸå›¾ * mask
        extracted[:, :, :3] = image_array * mask_bool[:, :, np.newaxis]  # âœ… ç°åœ¨éƒ½æ˜¯ numpy
        # Alpha é€šé“
        extracted[:, :, 3] = mask_bool * 255

        # è½¬ä¸º PIL å›¾åƒ
        cropped_img = Image.fromarray(extracted, mode="RGBA")

        # ã€å¯é€‰ã€‘è£å‰ªåˆ°ç‰©ä½“è¾¹ç•Œ
        coords = np.where(mask_bool)
        if len(coords[0]) == 0:
            continue
        top, left = coords[0].min(), coords[1].min()
        bottom, right = coords[0].max(), coords[1].max()

        # åŠ ç‚¹ margin
        margin = 10
        top = max(0, top - margin)
        left = max(0, left - margin)
        bottom = min(h, bottom + margin)
        right = min(w, right + margin)
        
        cropped_img = cropped_img.crop((left, top, right, bottom))
        cropped_img.save(f"tmp/{get_target_score(cropped_img,target)}.png")
    target=int(input("è¯·é€‰æ‹©ç¬¦åˆçš„:"))
    target_mask = masks[target]
    mask_np = target_mask.cpu().numpy() if isinstance(target_mask, torch.Tensor) else target_mask
    return mask_np>0.0

# -------------------------- æµ‹è¯•å…¥å£ï¼ˆæ”¯æŒä¸¤ç§æ¨¡å¼ï¼šä»…æ‰“åˆ† / å®Œæ•´åˆ†å‰²ï¼‰ --------------------------
if __name__ == "__main__":
    debug=False
    if debug:
        while True:
            x=input(":")
            print(get_target_score(Image.open("y.png").convert("RGB"),x))
    # åŠ è½½ä¾èµ–æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½ï¼Œåç»­æ— éœ€é‡å¤åŠ è½½ï¼‰
    # LoadLLM()  # è‹¥ä¸éœ€è¦LLMå¯æ³¨é‡Š
    LoadGroundingDINO()

    image_path = "data/2/0.jpg"
    image = Image.open(image_path).convert("RGB")

    while True:
        print("\nè¯·è¾“å…¥æ“ä½œæŒ‡ä»¤ï¼ˆæ ¼å¼ï¼šåŠŸèƒ½+ç›®æ ‡ï¼Œä¾‹ï¼šæ‰“åˆ† é¸Ÿ / åˆ†å‰² é¸Ÿï¼‰ï¼š")
        x = input(":")
        # æ¨¡å¼2ï¼šCLIP+GroundingDINO+SAMå®Œæ•´åˆ†å‰²
        mask = SAMForImage(image, x)
        if mask is None:
            print(f"æ— æ³•åˆ†å‰²'{x}'ï¼Œè¯·æ£€æŸ¥ç›®æ ‡æ˜¯å¦å­˜åœ¨")